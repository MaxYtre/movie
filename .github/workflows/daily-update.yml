name: Daily Cinema Calendar Update

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      CLEAR_CACHE:
        description: 'Delete data/cache.sqlite before run'
        required: false
        default: 'false'
      MAX_FILMS:
        description: 'Limit number of films to scan'
        required: false
        default: '200'
  push:
    branches: [ main ]
    paths:
      - 'movie_scraper/**'
      - '.github/workflows/**'

jobs:
  update-calendar:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ref: main
        persist-credentials: true
        fetch-depth: 0

    - name: Setup Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Prepare data dir and optionally clear cache
      run: |
        mkdir -p data
        if [ "${{ github.event.inputs.CLEAR_CACHE }}" = "true" ]; then
          echo "Clearing cache.sqlite by request"
          rm -f data/cache.sqlite || true
        fi
        if [ -f data/cache.sqlite ]; then echo "CACHE_PRESENT"; else echo "CACHE_ABSENT"; fi

    - name: Sanity check (imports & required functions)
      run: |
        python - <<'PY'
        import importlib, sys
        sys.path.insert(0, '.')
        m = importlib.import_module('movie_scraper.simple_scraper')
        required = ['parse_item_name']
        missing = [x for x in required if not hasattr(m, x)]
        if missing:
          raise SystemExit(f"Missing required functions: {missing}")
        print('Sanity OK')
        PY

    - name: Install dependencies
      run: |
        set -euo pipefail
        python -m pip install --upgrade pip
        pip install aiohttp beautifulsoup4 lxml icalendar python-dateutil pytz aiosqlite

    - name: Run scraper (TTL=15 days)
      env:
        PYTHONPATH: .
        MOVIE_SCRAPER_LOG_LEVEL: INFO
        MOVIE_SCRAPER_MAX_FILMS: "${{ github.event.inputs.MAX_FILMS || '200' }}"
        MOVIE_SCRAPER_RATE_LIMIT: "5.0"
        MOVIE_SCRAPER_CACHE_TTL_DAYS: "15"
        OMDB_API_KEY: ${{ secrets.OMDB_API_KEY }}
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        KINOPOISK_API_KEY: ${{ secrets.KINOPOISK_API_KEY }}
      run: |
        set -euo pipefail
        python -m py_compile movie_scraper/simple_scraper.py
        python - <<'PY'
        import asyncio, sys
        sys.path.insert(0, '.')
        from movie_scraper.simple_scraper import main
        asyncio.run(main())
        PY

    - name: Show DIAG COPY block
      if: always()
      run: |
        echo "--- DIAG COPY (main) ---"
        if [ -f docs/diag.txt ]; then tail -n 300 docs/diag.txt; else echo "no diag.txt"; fi
        echo "--- END DIAG ---"

    - name: List artifacts and cache
      if: always()
      run: |
        ls -la docs || true
        ls -la data || true
        test -f docs/calendar.ics && echo "ICS_OK" || echo "ICS_MISS"
        test -f docs/index.html && echo "INDEX_OK" || echo "INDEX_MISS"
        test -f data/cache.sqlite && echo "CACHE_OK" || echo "CACHE_MISS"

    - name: Commit and push docs and cache to main (rebase-safe)
      if: github.ref == 'refs/heads/main'
      run: |
        set -euo pipefail
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add docs/ data/cache.sqlite
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update docs & cache.sqlite (TTL=15d)"
          git fetch origin main
          if ! git rebase origin/main; then
            echo "Rebase failed; using 'ours' to keep local docs/data"
            git rebase --abort || true
            git merge -s ours -m "Resolve conflicts by keeping workflow artifacts" origin/main
          fi
          git push origin HEAD:main
        fi
