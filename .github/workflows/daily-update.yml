name: Daily Cinema Calendar Update

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      CLEAR_CACHE:
        description: 'Delete data/cache.sqlite before run'
        required: false
        default: 'false'
      MAX_FILMS:
        description: 'Limit number of films to scan'
        required: false
        default: '200'
  push:
    branches: [ main ]
    paths:
      - 'movie_scraper/**'
      - '.github/workflows/**'

jobs:
  update-calendar:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ref: main
        persist-credentials: true
        fetch-depth: 0

    - name: Setup Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        set -euo pipefail
        echo "[WF] Installing pip packages…"
        python -m pip install --upgrade pip
        pip install aiohttp beautifulsoup4 lxml icalendar python-dateutil pytz aiosqlite
        echo "[WF] Pip packages installed."

    - name: Prepare data dir and optionally clear cache
      run: |
        echo "[WF] Preparing data dir…"
        mkdir -p data
        if [ "${{ github.event.inputs.CLEAR_CACHE }}" = "true" ]; then
          echo "[WF] Clearing cache.sqlite by request"
          rm -f data/cache.sqlite || true
        fi
        if [ -f data/cache.sqlite ]; then echo "[WF] CACHE_PRESENT"; else echo "[WF] CACHE_ABSENT"; fi

    - name: Sanity check (imports & required functions)
      run: |
        echo "[WF] Running sanity check…"
        python - <<'PY'
        import importlib, sys
        sys.path.insert(0, '.')
        m = importlib.import_module('movie_scraper.simple_scraper')
        required = ['parse_item_name']
        missing = [x for x in required if not hasattr(m, x)]
        if missing:
          raise SystemExit(f"Missing required functions: {missing}")
        print('[WF] Sanity OK: simple_scraper loads and required functions found')
        PY

    - name: Run scraper (TTL=15 days) with stage logs
      env:
        PYTHONPATH: .
        MOVIE_SCRAPER_LOG_LEVEL: INFO
        MOVIE_SCRAPER_MAX_FILMS: "${{ github.event.inputs.MAX_FILMS || '200' }}"
        MOVIE_SCRAPER_RATE_LIMIT: "5.0"
        MOVIE_SCRAPER_CACHE_TTL_DAYS: "15"
        OMDB_API_KEY: ${{ secrets.OMDB_API_KEY }}
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        KINOPOISK_API_KEY: ${{ secrets.KINOPOISK_API_KEY }}
      run: |
        set -euo pipefail
        echo "[WF] Compiling Python…"
        python -m py_compile movie_scraper/simple_scraper.py
        echo "[WF] Starting scraper main()…"
        python - <<'PY'
        import asyncio, sys, time
        sys.path.insert(0, '.')
        from movie_scraper.simple_scraper import scrape, write_ics, write_index, write_diag, BASE
        from pathlib import Path
        from datetime import date
        from urllib.parse import urljoin

        async def run():
          docs = Path('docs'); docs.mkdir(exist_ok=True)
          diag = []
          print('[STAGE] BOOT')
          t0 = time.time()
          films, stats = await scrape()
          print(f"[STAGE] SCRAPE_DONE candidates={len(films)} elapsed={time.time()-t0:.1f}s")
          preview = []
          for f in films[:10]:
            preview.append(f"%s | %s | %s | %s | %s" % (f.title, f.next_date, f.country, f.age_limit or '', urljoin(BASE, f"/prm/schedule_cinema_product/{f.slug}/")))
          print('[STAGE] WRITE_ICS start')
          ics_path = write_ics(films, docs)
          print(f"[STAGE] WRITE_ICS done path={ics_path} size={ics_path.stat().st_size if ics_path.exists() else 0}")
          print('[STAGE] WRITE_INDEX start')
          write_index(docs, len(films), preview, stats)
          print('[STAGE] WRITE_INDEX done')
          print('[STAGE] WRITE_DIAG start')
          diag.append(f"SUMMARY candidates_processed={len(films)}")
          diag.append('=== DIAG COPY START ===')
          diag.append(f"limit={{{}}} foreign_films={len(films)} 429={stats['429']} 403={stats['403']} cache_hits={stats['cache_hits']} cache_misses={stats['cache_misses']} sleep_total={stats['sleep_total']:.1f}".format('${{ env.MOVIE_SCRAPER_MAX_FILMS }}'))
          write_diag(docs, diag)
          print('[STAGE] WRITE_DIAG done')
        asyncio.run(run())
        PY
        echo "[WF] Scraper finished."

    - name: Show DIAG COPY block
      if: always()
      run: |
        echo "--- DIAG COPY (main) ---"
        if [ -f docs/diag.txt ]; then tail -n 300 docs/diag.txt; else echo "no diag.txt"; fi
        echo "--- END DIAG ---"

    - name: List artifacts and cache
      if: always()
      run: |
        ls -la docs || true
        ls -la data || true
        test -f docs/calendar.ics && echo "ICS_OK" || echo "ICS_MISS"
        test -f docs/index.html && echo "INDEX_OK" || echo "INDEX_MISS"
        test -f data/cache.sqlite && echo "CACHE_OK" || echo "CACHE_MISS"

    - name: Commit and push docs and cache to main (rebase-safe)
      if: github.ref == 'refs/heads/main'
      run: |
        set -euo pipefail
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add docs/ data/cache.sqlite
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update docs & cache.sqlite (TTL=15d)"
          git fetch origin main
          if ! git rebase origin/main; then
            echo "Rebase failed; using 'ours' to keep local docs/data"
            git rebase --abort || true
            git merge -s ours -m "Resolve conflicts by keeping workflow artifacts" origin/main
          fi
          git push origin HEAD:main
        fi
