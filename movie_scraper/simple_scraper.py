"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏.

–°–æ–∑–¥–∞–µ—Ç ICS –∫–∞–ª–µ–Ω–¥–∞—Ä—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ –¥–ª—è GitHub Pages.
"""

import asyncio
import json
import logging
import re
from datetime import date
from datetime import datetime
from datetime import timedelta
from pathlib import Path
from typing import Dict
from typing import List
from typing import Optional
from urllib.parse import urljoin

import aiohttp
from bs4 import BeautifulSoup
from icalendar import Calendar
from icalendar import Event

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FilmInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–ª—å–º–µ."""
    
    def __init__(self, title: str, country: str, url: str, next_date: Optional[date] = None):
        self.title = title
        self.country = country
        self.url = url
        self.next_date = next_date
        self.description = ""
        self.rating = ""
        self.age_limit = ""
        
    @property
    def is_foreign(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∏–ª—å–º –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–º."""
        russian_keywords = ['—Ä–æ—Å—Å–∏—è', 'russia', '—Ä—Ñ', '—Å—Å—Å—Ä', 'ussr']
        return not any(keyword in self.country.lower() for keyword in russian_keywords)
    
    def __repr__(self) -> str:
        return f"FilmInfo({self.title}, {self.country}, foreign={self.is_foreign})"


class PermCinemaScraper:
    """–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è –∞—Ñ–∏—à–∞.ru –ü–µ—Ä–º—å."""
    
    def __init__(self):
        self.base_url = "https://www.afisha.ru"
        self.perm_url = f"{self.base_url}/prm/schedule_cinema/"
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            },
            connector=connector,
            timeout=timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def _fetch_with_retry(self, url: str, retries: int = 3) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        for attempt in range(retries):
            try:
                logger.info(f"Fetching {url} (attempt {attempt + 1})")
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status == 429:
                        # Rate limited - wait longer
                        await asyncio.sleep(2 ** attempt)
                        continue
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        
            except Exception as e:
                logger.warning(f"Request failed: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(1 * (attempt + 1))
                
        return None
    
    async def scrape_film_listings(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Ñ–∏–ª—å–º–æ–≤."""
        film_urls = []
        page = 1
        
        while page <= 5:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            if page == 1:
                url = self.perm_url
            else:
                url = f"{self.perm_url}page{page}/"
            
            html = await self._fetch_with_retry(url)
            if not html:
                break
            
            soup = BeautifulSoup(html, 'html.parser')
            page_urls = self._extract_film_urls(soup)
            
            if not page_urls:
                logger.info(f"No films found on page {page}, stopping")
                break
                
            film_urls.extend(page_urls)
            logger.info(f"Found {len(page_urls)} films on page {page}")
            page += 1
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)
        
        return list(set(film_urls))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _extract_film_urls(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ñ–∏–ª—å–º–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è."""
        urls = []
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–∏–ª—å–º—ã
        selectors = [
            'a[href*="/prm/schedule_cinema_product/"]',
            'a[href*="/schedule_cinema_product/"]',
            '.b-object-item h3 a',
            '.film-title a',
            'h3 a[href*="cinema"]',
            '.object-summary-title-link'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if href and '/schedule_cinema_product/' in href:
                    full_url = urljoin(self.base_url, href)
                    urls.append(full_url)
        
        return urls
    
    async def get_film_info(self, film_url: str) -> Optional[FilmInfo]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–ª—å–º–µ."""
        html = await self._fetch_with_retry(film_url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        title = self._extract_title(soup)
        if not title:
            return None
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–∞–Ω—É
        country = self._extract_country(soup)
        if not country:
            return None
            
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ñ–∏–ª—å–º–∞
        film = FilmInfo(title, country, film_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∏–ª—å–º –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–º
        if not film.is_foreign:
            logger.debug(f"Skipping Russian film: {title} ({country})")
            return None
            
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Å–µ–∞–Ω—Å
        film.next_date = self._find_next_screening(soup)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        film.description = self._extract_description(soup)
        film.rating = self._extract_rating(soup)
        film.age_limit = self._extract_age_limit(soup)
        
        return film
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞."""
        selectors = ['h1', '.object-summary-title', '.film-title', '.b-object-title']
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        return None
    
    def _extract_country(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–∞–Ω—É –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞."""
        # –ò—â–µ–º –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∏–ª—å–º–µ
        text_content = soup.get_text()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω—ã
        patterns = [
            r'–°—Ç—Ä–∞–Ω–∞[:\s]+([^\n,]+)',
            r'—Å—Ç—Ä–∞–Ω–∞[:\s]+([^\n,]+)',
            r'Country[:\s]+([^\n,]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text_content, re.IGNORECASE)
            if match:
                country = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                country = re.sub(r'[\r\n\t]+', ' ', country)
                return country
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        meta_selectors = [
            '[itemprop="countryOfOrigin"]',
            '.country',
            '.film-country',
            '.object-country'
        ]
        
        for selector in meta_selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        # –§–æ–ª–ª–±—ç–∫: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä–∞–Ω—É, —Å—á–∏—Ç–∞–µ–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–º
        logger.warning(f"Could not determine country, assuming Russian")
        return "–†–æ—Å—Å–∏—è"
    
    def _find_next_screening(self, soup: BeautifulSoup) -> Optional[date]:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥–∞—Ç—É –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–µ–∞–Ω—Å–∞."""
        today = date.today()
        
        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
        text = soup.get_text()
        date_patterns = [
            r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)',
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})'
        ]
        
        found_dates = []
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    if len(match) == 2 and isinstance(match[1], str):  # –º–µ—Å—è—Ü —Å–ª–æ–≤–æ–º
                        day, month_name = match
                        month_map = {
                            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
                            '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
                            '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
                        }
                        if month_name in month_map:
                            screening_date = date(today.year, month_map[month_name], int(day))
                            if screening_date >= today:
                                found_dates.append(screening_date)
                except ValueError:
                    continue
        
        if found_dates:
            return min(found_dates)  # –ë–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–∞—Ç—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≤—Ç—Ä–∞
        return today + timedelta(days=1)
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞."""
        selectors = ['.annotation', '.description', '.film-description', '.b-object-lead']
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                desc = elem.get_text(strip=True)
                return desc[:300] + "..." if len(desc) > 300 else desc
        
        return ""
    
    def _extract_rating(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ —Ñ–∏–ª—å–º–∞."""
        text = soup.get_text()
        
        # –ò—â–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏
        rating_patterns = [
            r'IMDb[:\s]+(\d+\.\d+)',
            r'–ö–∏–Ω–æ–ø–æ–∏—Å–∫[:\s]+(\d+\.\d+)',
            r'—Ä–µ–π—Ç–∏–Ω–≥[:\s]+(\d+\.\d+)',
        ]
        
        for pattern in rating_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ""
    
    def _extract_age_limit(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ."""
        text = soup.get_text()
        
        age_patterns = [
            r'(\d+\+)',
            r'–≤–æ–∑—Ä–∞—Å—Ç[:\s]+(\d+\+)',
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ""


class ICSCalendarGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä ICS –∫–∞–ª–µ–Ω–¥–∞—Ä—è."""
    
    @staticmethod
    def generate_calendar(films: List[FilmInfo]) -> bytes:
        """–°–æ–∑–¥–∞–µ—Ç ICS –∫–∞–ª–µ–Ω–¥–∞—Ä—å –∏–∑ —Å–ø–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤."""
        cal = Calendar()
        cal.add('prodid', '-//Perm Foreign Films//perm-cinema//EN')
        cal.add('version', '2.0')
        cal.add('calscale', 'GREGORIAN')
        cal.add('method', 'PUBLISH')
        cal.add('x-wr-calname', '–ó–∞—Ä—É–±–µ–∂–Ω—ã–µ —Ñ–∏–ª—å–º—ã –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–∞—Ö –ü–µ—Ä–º–∏')
        cal.add('x-wr-caldesc', '–ò–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã, –∏–¥—É—â–∏–µ –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–∞—Ö –ü–µ—Ä–º–∏. –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ.')
        
        for film in films:
            if not film.next_date:
                continue
                
            event = Event()
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            slug = film.url.split('/')[-2] if film.url.endswith('/') else film.url.split('/')[-1]
            event.add('uid', f"{slug}-{film.next_date.isoformat()}@perm-cinema")
            
            # –î–∞—Ç—ã (–≤–µ—Å—å –¥–µ–Ω—å)
            event.add('dtstart', film.next_date)
            event.add('dtend', film.next_date)
            event.add('dtstamp', datetime.now())
            
            # –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ —Å–æ–±—ã—Ç–∏–µ –Ω–∞ –≤–µ—Å—å –¥–µ–Ω—å
            event['dtstart'].params['VALUE'] = 'DATE'
            event['dtend'].params['VALUE'] = 'DATE'
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
            title = film.title
            if film.age_limit:
                title += f" ({film.age_limit})"
            event.add('summary', title)
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            description_parts = []
            if film.country:
                description_parts.append(f"–°—Ç—Ä–∞–Ω–∞: {film.country}")
            if film.rating:
                description_parts.append(f"–†–µ–π—Ç–∏–Ω–≥: {film.rating}")
            if film.description:
                description_parts.append(f"\n–û–ø–∏—Å–∞–Ω–∏–µ: {film.description}")
            if film.url:
                description_parts.append(f"\n–ü–æ–¥—Ä–æ–±–Ω–µ–µ: {film.url}")
            
            event.add('description', '\n'.join(description_parts))
            
            # URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if film.url:
                event.add('url', film.url)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            event.add('categories', ['–ó–ê–†–£–ë–ï–ñ–ù–´–ï-–§–ò–õ–¨–ú–´', '–ö–ò–ù–û', '–ü–ï–†–ú–¨'])
            
            cal.add_component(event)
        
        return cal.to_ical()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∞–ø–µ—Ä–∞."""
    logger.info("Starting Perm cinema scraper...")
    
    try:
        async with PermCinemaScraper() as scraper:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏–ª—å–º–æ–≤
            logger.info("Fetching film listings...")
            film_urls = await scraper.scrape_film_listings()
            logger.info(f"Found {len(film_urls)} films to process")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∏–ª—å–º—ã
            foreign_films = []
            
            for i, url in enumerate(film_urls[:20], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                logger.info(f"Processing film {i}/{min(20, len(film_urls))}: {url}")
                
                film_info = await scraper.get_film_info(url)
                if film_info and film_info.is_foreign:
                    foreign_films.append(film_info)
                    logger.info(f"Added foreign film: {film_info.title} ({film_info.country})")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)
            
            logger.info(f"Found {len(foreign_films)} foreign films")
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞–ª–µ–Ω–¥–∞—Ä—å
            if foreign_films:
                ics_data = ICSCalendarGenerator.generate_calendar(foreign_films)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                docs_dir = Path("docs")
                docs_dir.mkdir(exist_ok=True)
                
                calendar_path = docs_dir / "calendar.ics"
                with open(calendar_path, 'wb') as f:
                    f.write(ics_data)
                
                logger.info(f"Calendar saved to {calendar_path} with {len(foreign_films)} films")
                
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                create_index_page(docs_dir, len(foreign_films))
                
            else:
                logger.warning("No foreign films found, not generating calendar")
                
    except Exception as e:
        logger.error(f"Scraper failed: {e}")
        raise


def create_index_page(docs_dir: Path, film_count: int):
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è GitHub Pages."""
    html_content = f"""
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–ö–∞–ª–µ–Ω–¥–∞—Ä—å –∑–∞—Ä—É–±–µ–∂–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ –≤ –ü–µ—Ä–º–∏</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .calendar-link {{
            display: inline-block;
            background: #4CAF50;
            color: white;
            padding: 12px 24px;
            text-decoration: none;
            border-radius: 4px;
            margin: 10px 0;
        }}
        .calendar-link:hover {{
            background: #45a049;
        }}
        .instructions {{
            background: #e8f5e8;
            padding: 20px;
            border-radius: 4px;
            margin: 20px 0;
        }}
        .stats {{
            color: #666;
            font-size: 14px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üé¨ –ö–∞–ª–µ–Ω–¥–∞—Ä—å –∑–∞—Ä—É–±–µ–∂–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ –≤ –ü–µ—Ä–º–∏</h1>
        <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤, –∏–¥—É—â–∏—Ö –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–∞—Ö –ü–µ—Ä–º–∏.</p>
        
        <a href="calendar.ics" class="calendar-link" download>üìÖ –°–∫–∞—á–∞—Ç—å –∫–∞–ª–µ–Ω–¥–∞—Ä—å (.ics)</a>
        
        <div class="instructions">
            <h3>–ö–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –∫ Google Calendar:</h3>
            <ol>
                <li>–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å—Å—ã–ª–∫—É: <code>https://maxytree.github.io/movie/calendar.ics</code></li>
                <li>–û—Ç–∫—Ä–æ–π—Ç–µ Google Calendar</li>
                <li>–°–ª–µ–≤–∞ –Ω–∞–∂–º–∏—Ç–µ "+" —Ä—è–¥–æ–º —Å "–î—Ä—É–≥–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–∏"</li>
                <li>–í—ã–±–µ—Ä–∏—Ç–µ "–ò–∑ URL"</li>
                <li>–í—Å—Ç–∞–≤—å—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Å—ã–ª–∫—É</li>
                <li>–ù–∞–∂–º–∏—Ç–µ "–î–æ–±–∞–≤–∏—Ç—å –∫–∞–ª–µ–Ω–¥–∞—Ä—å"</li>
            </ol>
        </div>
        
        <div class="instructions">
            <h3>–î–ª—è –¥—Ä—É–≥–∏—Ö –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:</h3>
            <ul>
                <li><strong>Apple Calendar:</strong> –§–∞–π–ª ‚Üí –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –∫–∞–ª–µ–Ω–¥–∞—Ä—å</li>
                <li><strong>Outlook:</strong> –î–æ–±–∞–≤–∏—Ç—å –∫–∞–ª–µ–Ω–¥–∞—Ä—å ‚Üí –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –∏–∑ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–∞</li>
                <li><strong>–î—Ä—É–≥–∏–µ:</strong> –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª .ics –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –µ–≥–æ</li>
            </ul>
        </div>
        
        <div class="stats">
            <p>üìä –°–µ–π—á–∞—Å –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ: {film_count} –∑–∞—Ä—É–±–µ–∂–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤</p>
            <p>üîÑ –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 11:00 –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ü–µ—Ä–º–∏</p>
            <p>üìÖ –°–æ–±—ã—Ç–∏—è —Å–æ–∑–¥–∞—é—Ç—Å—è –Ω–∞ –≤–µ—Å—å –¥–µ–Ω—å (–±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–µ–∞–Ω—Å–∞)</p>
            <p>‚è∞ –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {datetime.now().strftime('%d.%m.%Y –≤ %H:%M')}</p>
        </div>
        
        <hr>
        <p><small>
            –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: <a href="https://www.afisha.ru/prm/schedule_cinema/" target="_blank">afisha.ru</a> | 
            –ö–æ–¥ –ø—Ä–æ–µ–∫—Ç–∞: <a href="https://github.com/MaxYtre/movie" target="_blank">GitHub</a>
        </small></p>
    </div>
</body>
</html>
"""
    
    index_path = docs_dir / "index.html"
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    logger.info(f"Index page created at {index_path}")


if __name__ == "__main__":
    asyncio.run(main())